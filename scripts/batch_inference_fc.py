#!/usr/bin/env python3
"""
Script: batch_inference_fc.py
- Uses LiteLLM with OpenAI Native Function Calling (Tools API).
- Loads 'description' directly from tools.json for fair comparison.
- Filters out Navigation and low-level tools (motion_delta, etc.).
- Forces tool selection via tool_choice='required'.
- Handles Rate Limits with Exponential Backoff + Jitter.
"""

import argparse
import json
import os
import sys
import asyncio
import random
import re
from tqdm.asyncio import tqdm_asyncio
from litellm import acompletion, RateLimitError 

# ************************************************
# âš ï¸ ë³´ì•ˆ ê²½ê³ : ì‹¤ì œ API í‚¤ë¡œ êµì²´í•˜ì„¸ìš”
API_KEY = ""
# ************************************************
 
# --- Configuration ---
API_MODEL = "openai/gpt4o" 
MAX_CONCURRENT_REQUESTS = 1   # Rate Limitì— ë”°ë¼ ì¡°ì ˆ (ì—ëŸ¬ ë§ìœ¼ë©´ 1~2ë¡œ ë‚®ì¶¤)
SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
MAX_RETRIES = 5
BASE_DELAY = 5

# --- Target Tools Filter ---
# ì´ ëª©ë¡ì— ìˆëŠ” ë„êµ¬ë§Œ tools.jsonì—ì„œ ê°€ì ¸ì™€ APIì— ì „ë‹¬í•©ë‹ˆë‹¤. (Navigation, motion_delta ì œì™¸ë¨)
TARGET_TOOL_NAMES = {
    "Pickup", "Place", "Open", "Close",
    "Slice", "Dirty", "Clean", "ToggleOn", "ToggleOff",
    "Fill", "Empty", "Pour", "Break"
}

safety_settings = [
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    # ... ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬
]

def log(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def load_tools_schema_from_file(json_path):
    """
    tools.json íŒŒì¼ì„ ì½ì–´ì„œ OpenAI Function Calling Schemaë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    Descriptionì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ Prompt ë°©ì‹ê³¼ ê³µì •í•˜ê²Œ ë¹„êµí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(json_path):
        log(f"[ERROR] Tools file not found: {json_path}")
        sys.exit(1)

    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    openai_tools = []
    
    # tools.jsonì˜ "tools" ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ
    for tool in data.get("tools", []):
        tool_name = tool.get("name")
        
        # 1. íƒ€ê²Ÿ ë„êµ¬ì¸ì§€ í™•ì¸ (Navigation ë“± ì œì™¸)
        if tool_name in TARGET_TOOL_NAMES:
            # 2. Schema ìƒì„±
            schema = {
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool.get("description", ""), # ğŸ‘ˆ í•µì‹¬: ì›ë³¸ ì„¤ëª… ì‚¬ìš©
                    "parameters": {
                        "type": "object",
                        "properties": {}, 
                        "required": []
                    }
                }
            }
            openai_tools.append(schema)
            
    log(f"[INFO] Loaded {len(openai_tools)} tools from {json_path}")
    return openai_tools

def parse_tool_calls(response_message):
    """
    API ì‘ë‹µì—ì„œ tool_callsë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸°ì¡´ í¬ë§·ì¸ {"actions": [...]}ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not hasattr(response_message, 'tool_calls') or not response_message.tool_calls:
        return None

    actions = []
    for tool_call in response_message.tool_calls:
        function_name = tool_call.function.name
        try:
            # ì¸ì íŒŒì‹± (ë³´í†µ ë¹ˆ ê°ì²´ {} ì„)
            function_args = json.loads(tool_call.function.arguments)
        except:
            function_args = {}
            
        actions.append({
            "tool_name": function_name,
            "parameters": function_args
        })
    
    if not actions: return None
    return {"actions": actions}

def get_all_jsonl_files(root_dir):
    jsonl_files = []
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f.endswith('.jsonl'):
                jsonl_files.append(os.path.join(root, f))
    return sorted(jsonl_files)

async def process_single_file(input_path, args, tools_schema):
    relative_path = os.path.relpath(input_path, args.input_dir)
    output_path = os.path.join(args.output_dir, relative_path)
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
    if os.path.exists(output_path): return

    # í´ë” ìƒì„±
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    try:
        # ì…ë ¥ íŒŒì¼ ì½ê¸°
        with open(input_path, 'r', encoding='utf-8') as fh:
            line = fh.readline()
            if not line.strip(): return
            sample = json.loads(line)

        if 'prompt' not in sample: return
        prompt_content = sample['prompt']
        
        final_prediction = None
        raw_response_content = ""
        
        # ì¬ì‹œë„ ë£¨í”„
        for attempt in range(MAX_RETRIES):
            async with SEMAPHORE:
                try:
                    response = await acompletion(
                        model=args.model_name,
                        messages=[
                            {"role": "system", "content": "You are a robot control assistant. Select the correct function based on the context."},
                            {"role": "user", "content": prompt_content}
                        ],
                        temperature=1,
                        max_tokens=1024,
                        api_key=API_KEY,
                        tools=tools_schema,      # ğŸ‘ˆ íŒŒì¼ì—ì„œ ë¡œë“œí•œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
                        tool_choice="required",   # ğŸ‘ˆ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš© ê°•ì œ
                    )
                    
                    message = response.choices[0].message
                    raw_response_content = str(message) # ë””ë²„ê¹…ìš© ì „ì²´ ì‘ë‹µ ì €ì¥
                    final_prediction = parse_tool_calls(message)
                    break 
                    
                except RateLimitError as e:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°
                    delay = BASE_DELAY * (2 ** attempt) + random.uniform(0, 1) 
                    log(f"\n[RATE LIMIT] File: {os.path.basename(input_path)}. Attempt {attempt+1}/{MAX_RETRIES}. Waiting {delay:.2f}s...")
                    
                    if attempt < MAX_RETRIES - 1:
                        await asyncio.sleep(delay)
                    else:
                        raw_response_content = f"API_ERROR: RateLimitError persisted. {e}"
                        break
                except Exception as e:
                    raw_response_content = f"API_ERROR: {e}"
                    log(f"[CRITICAL API ERROR] {input_path}: {e}")
                    break

        # ê²°ê³¼ ì €ì¥ êµ¬ì¡°
        result_entry = {
            "game_id": sample.get("game_id"),
            "instance_id": sample.get("instance_id"),
            "origin_file": input_path,
            "model_name": args.model_name,
            "model_output_raw": raw_response_content,
            "prediction": final_prediction,
            "ground_truth": sample.get("answer", {})
        }

        # íŒŒì¼ ì“°ê¸°
        with open(output_path, 'w', encoding='utf-8') as out_fh:
            out_fh.write(json.dumps(result_entry, ensure_ascii=False) + '\n')

    except Exception as e:
        log(f"[FATAL ERROR] processing {input_path}: {e}")

DEFAULT_IDS_FILE = os.path.join(os.path.dirname(__file__), 'selected_500_instance_ids.json')


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-name', default=API_MODEL)
    parser.add_argument('--input-dir', required=True)
    parser.add_argument('--output-dir', required=True)
    parser.add_argument('--tools-file', required=True, help="Path to tools.json file") # í•„ìˆ˜ ì¸ì
    parser.add_argument('--ids-file', default=DEFAULT_IDS_FILE, help="Optional: JSON list of instance_ids to process (defaults to selected_500_instance_ids.json in this scripts folder)") 
    
    args = parser.parse_args()

    if API_KEY == "YOUR_GPT_4O_API_KEY_HERE":
        log("[FATAL] Please update the API_KEY variable at the top of the script!")
        sys.exit(1)

    # 1. ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
    tools_schema = load_tools_schema_from_file(args.tools_file)

    # 2. íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    input_files = get_all_jsonl_files(args.input_dir)
    log(f'[INFO] Found {len(input_files)} total files.')

    # 3. (ì˜µì…˜) íŠ¹ì • IDë§Œ í•„í„°ë§ - ê¸°ë³¸ìœ¼ë¡œ scripts/selected_500_instance_ids.json ì‚¬ìš©
    if args.ids_file:
        if os.path.exists(args.ids_file):
            with open(args.ids_file, 'r') as f:
                target_ids = set(json.load(f))
            
            filtered_files = []
            for p in input_files:
                try:
                    with open(p, 'r') as fh:
                        line = fh.readline()
                        if not line: continue
                        data = json.loads(line)
                        if data.get("instance_id") in target_ids:
                            filtered_files.append(p)
                except: continue
            input_files = filtered_files
            log(f'[INFO] Filtered to {len(input_files)} files based on ids-file.')
        else:
            log(f'[WARNING] ids-file provided but not found: {args.ids_file}. Processing all files instead.')

    if not input_files:
        log('[INFO] No files to process.')
        return

    log(f'[INFO] Starting FC Inference. Model: {args.model_name}')

    # 4. ì‘ì—… ì‹œì‘
    tasks = [process_single_file(f, args, tools_schema) for f in input_files]
    await tqdm_asyncio.gather(*tasks, desc=f"FC Inference ({args.model_name})")

if __name__ == '__main__':
    asyncio.run(main())